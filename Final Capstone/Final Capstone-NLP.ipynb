{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial imports\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "#BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "#gensim\n",
    "import bz2\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pickle\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "from gensim.models import Phrases\n",
    "#NLTK\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from  plotly  import __version__\n",
    "#plotly offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "print(__version__) # requires version >= 1.9.0\n",
    "init_notebook_mode(connected=True)\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import linear_model\n",
    "#SpaCy\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Desktop/Final Capstone/jobs_db.csv')\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn our df into X, for later use with modelling, train_test_split and getting a y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we web scraped, the text is in pretty raw format.  To get it into useable shape, I plan to get rid of the html code before tokenizing and lemmatizing it. I'll also check for duplicates, and remove any to keep the results as clean as possible.  From here, it will be easier to determine what words can be added as stop words, or, are meaningful to the analysis in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check for unique job descriptions. \n",
    "#If we drop some duplicates, we'll run our cleaning on less data, which is faster.\n",
    "X.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.job_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby(X['job_class'])['job_description'].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It stands to reason that we would want to keep the unique job descriptions, because titles like \"Data Scientist\"\n",
    "#are broadly used in the field and not necessarily problematic for the scope of this project.\n",
    "X = X.drop_duplicates(['job_description'], keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check to make sure we dropped what we had intended.\n",
    "X.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby(X['job_class'])['job_description'].count().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of our \"female dominated fields is 1,500 which is 34.6% of our dataset. I'm going to drop rows that are Computer Vision job descriptions.  This should help even out the imbalance, and that particular field was the most \"gender-balanced\" out of the male-dominated job descriptions we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Computer Vision\n",
    "X = X[X.job_class != 'Computer Vision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby(X['job_class'])['job_description'].count().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we lost a large portion of the dataset due to duplicates, it's important to prioritize quality over quantity. I evaluated the dataset to ensure that there would not be a class imbalance problem.  Now, our female-dominated fields comprise 40% of the job descriptions, this is pretty good considering we're working with scraped data. So, we'll proceed with cleaning up the dataset, so we can put it into BoW and Tf-Idf and then run some models on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quick view of the dataset to see what we might need to clean.\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of HTML script here, as expected. Let's get some of that cleaned up before we tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].replace(r'div class=\"jobsearch-JobComponent-description icl-u-xs-mt--md\"><div></div><div><div><div><b>', '', regex=True).replace(r'</div', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['title'] = X['title'].replace(r'<h3 class=\"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title\">', '', regex=True).replace(r'</h3>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, our job titles look really nice at this point. Onward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].replace(r'<div class=\"jobsearch-JobComponent-description icl-u-xs-mt--md\">', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will get rid of most of the html markers that are within <>\n",
    "def clean_html(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for text in X:\n",
    "    for i in range(len(X)):\n",
    "        X['job_description'][i] = clean_html(X['job_description'][i])\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked like a charm! Let's continue cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].replace(r'\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].replace(r'&amp;', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['title'] = X['title'].replace(r'&amp;', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].replace(r'>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last regex left quite a bit of whitespace, which isn't useful. Let's strip that before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['job_description'] = X['job_description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't need the ids that populated in the SQLite Database\n",
    "X = X.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove punctuation.\n",
    "def punctuation_remover(row):\n",
    "    punctuation = '!\"#$&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~«»'  \n",
    "    \n",
    "    punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\", punctuation) + \"]\"\n",
    "    row = re.sub(punctuationNoPeriod, \" \", str(row))\n",
    "    #to remove double white spaces and create space after %,\n",
    "    row = row.replace('  ', ' ')\n",
    "    row = row.replace('%', '% ')\n",
    "    return row.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from the job titles first\n",
    "X['title'] = X['title'].apply(punctuation_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since that worked well, let's now remove punctuation from the job descriptions.\n",
    "X['job_description'] = X['job_description'].apply(punctuation_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['gender_breakdown'] = X['job_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.job_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X['gender_breakdown'] == 'Natural Language Processing', 'gender_breakdown'] = 1\n",
    "X.loc[X['gender_breakdown'] == 'Text Mining', 'gender_breakdown'] = 1\n",
    "X.loc[X['gender_breakdown'] == 'Text Analytics', 'gender_breakdown'] = 1\n",
    "X.loc[X['gender_breakdown'] == 'Speech Recognition', 'gender_breakdown'] = 1\n",
    "X.loc[X['gender_breakdown'] == 'Pattern Recognition', 'gender_breakdown'] = 0\n",
    "X.loc[X['gender_breakdown'] == 'Machine Learning', 'gender_breakdown'] = 0\n",
    "X.loc[X['gender_breakdown'] == 'Apache Spark', 'gender_breakdown'] = 0\n",
    "X.loc[X['gender_breakdown'] == 'Neural Networks', 'gender_breakdown'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the Latend Dirichlet Allocation method first, and then use it with bigrams and trigrams. I will evalute for coherence, and tune the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step, tokenizer.\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = en_stop.union(set(['machine','business','analytics','learning','technology',\n",
    "                              'system', 'science', 'research', 'analysis', 'experience',\n",
    "                               'customer', 'include', 'plant', 'clerkship', 'language',\n",
    "                               'university', 'google', 'hadoop', 'spark', 'apache', 'microsoft',\n",
    "                               'ai', 'ml', 'nlp', 'speech', 'natural', 'engineers', 'cloud',\n",
    "                               'intelligence', 'aws']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "text_data = []\n",
    "for text in X:\n",
    "    for i in range(len(X)):\n",
    "        tokens = prepare_text_for_lda(X['job_description'][i])\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gensim to bolster NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model3.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 5, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "# Convert to array\n",
    "docs =array(X['job_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for tokenize and lemmatizing\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform function on our document\n",
    "docs = docs_preprocessor(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Biagram & Trigram Models \n",
    "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare & common tokens \n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary and corpus required for Topic Modeling\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters.\n",
    "num_topics = 5\n",
    "chunksize = 500 \n",
    "passes = 20 \n",
    "iterations = 400\n",
    "eval_every = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=2, limit=40, step=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is excellent coherence at the 5 topics I had used, so we will stay with that, in an effort to avoid being overly selective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A (Quasi) Supervised Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW with SKLearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = en_stop.union(set(['machine','business','analytics','learning','technology',\n",
    "                              'system', 'science', 'research', 'analysis', 'experience',\n",
    "                               'customer', 'include', 'plant', 'clerkship', 'language',\n",
    "                               'university', 'google', 'hadoop', 'spark', 'apache', 'microsoft',\n",
    "                               'ai', 'ml', 'nlp', 'speech', 'natural', 'engineers', 'cloud',\n",
    "                               'intelligence', 'aws']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove HTML\n",
    "    soup = BeautifulSoup(text)\n",
    "    souped = soup.get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    lower_case = letters_only.lower() \n",
    "    words = tok.tokenize(lower_case)\n",
    "\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = stop_words                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    #return( \" \".join( meaningful_words ))   \n",
    "    \n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from the job titles first\n",
    "#X['job_description'] = X['job_description'].apply(clean_text)\n",
    "for text in X:\n",
    "    for i in range(len(X)):\n",
    "        X['job_description'][i] = clean_text(X['job_description'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up for classification\n",
    "y = X.job_class\n",
    "X1 = X.job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, analyzer='word', ngram_range=(1, 1), max_df=.30, min_df=.10, max_features=None)\n",
    "bow = vectorizer.fit_transform(X1)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into X_train and X_test now that BoW is complete.\n",
    "X_train, X_test= train_test_split(bow, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to test initial Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "fit = rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "start = time.time()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf = XGBClassifier()  \n",
    "clf.fit(X_train,y_train)  \n",
    "clf.predict(X_test)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "cross_val = cross_val_score(clf, X_train, y_train)\n",
    "print('Cross Validation Score:', cross_val)\n",
    "print('Cross Validation Mean:', cross_val.mean())\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BoW did not perform well, even with a boosted model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "X_norm = normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Reduce it to two components.\n",
    "X_pca_bow = PCA(2).fit_transform(X_norm.todense())\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=8, random_state=42).fit_predict(X_pca_bow)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca_bow[:, 0], X_pca_bow[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty nice clusters, with the exception of the variance of the top right green blob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)\n",
    "\n",
    "true_k = 10\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', batch_size=5000)\n",
    "\n",
    "km.fit(X_train_norm)\n",
    "km_train_label = km.labels_\n",
    "km_test_label = km.predict(X_test_norm)\n",
    "true_k = 10\n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', batch_size=5000)\n",
    "\n",
    "km.fit(X_train_norm)\n",
    "km_train_label = km.labels_\n",
    "km_test_label = km.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "fit = rfc.fit(X_train_norm, y_train)\n",
    "y_pred = rfc.predict(X_test_norm)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_norm, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test_norm, y_test))\n",
    "t0 = time()\n",
    "print(\"Done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "import time\n",
    "start = time.time()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_norm,y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train_norm, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_norm, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "import time\n",
    "start = time.time()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_norm,y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train_norm, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_norm, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=5,n_iter=500)\n",
    "lsa.fit(bow)\n",
    "terms = vectorizer.get_feature_names()\n",
    "X_train_lsa = lsa.fit_transform(X_train)\n",
    "X_test_lsa = lsa.transform(X_test)\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X1, y, test_size=0.25, random_state=42)\n",
    "count_vect_bigram = CountVectorizer(stop_words=stop_words, analyzer='word', ngram_range=(1,2), max_df=.30, min_df=.1, max_features=100000)\n",
    "train_data_bow_bigram = count_vect_bigram.fit_transform(X_train_clean)\n",
    "test_data_bow_bigram = count_vect_bigram.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = normalize(train_data_bow_bigram)\n",
    "# Reduce it to two components.\n",
    "X_pca_bow = PCA(2).fit_transform(X_norm.todense())\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=4, random_state=42).fit_predict(X_pca_bow)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca_bow[:, 0], X_pca_bow[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Natural Language Processing', 'Text Mining',\n",
    "         'Pattern Recognition', 'Machine Learning', 'Text Analytics',\n",
    "         'Apache Spark', 'Speech Recognition', 'Neural Networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#Confusion matrix for Bigram BOW with Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_data_bow_bigram,y_train)\n",
    "\n",
    "print('Training set score:', lr.score(train_data_bow_bigram, y_train_clean))\n",
    "print('\\nTest set score:', lr.score(test_data_bow_bigram, y_test_clean))\n",
    "\n",
    "y_pred = lr.predict(test_data_bow_bigram)\n",
    "\n",
    "conf = confusion_matrix(y_test_clean, y_pred, labels = labels)\n",
    "\n",
    "cm = pd.DataFrame(conf, index = [i for i in labels],\n",
    "                  columns = [i for i in labels])\n",
    "plt.figure(figsize = (15,7))\n",
    "plt.title('Confusion Matrix for BoW with Bigrams')\n",
    "sns.heatmap(cm, annot=True, cmap=\"PuBu\", fmt = 'd')\n",
    "crs = cross_val_score(lr, train_data_bow_bigram, y_train_clean, cv=10)\n",
    "print(\"Cross-Valid Bigram Fold Results are: \",crs)\n",
    "print(\"Mean of Folds are = \",crs.mean())\n",
    "end = time.time()\n",
    "print(\"Done in %0.3fs\", end - start)\n",
    "#conf = confusion_matrix(y_test_clean, bigram_pred, labels = labels)\n",
    "\n",
    "#df_cm = pd.DataFrame(conf, index = [i for i in labels],\n",
    "                  #columns = [i for i in labels])\n",
    "#plt.figure(figsize = (15,7))\n",
    "#sns.heatmap(df_cm, annot=True, cmap=\"Blues\", fmt = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(silent=True, \n",
    "                    scale_pos_weight=1,\n",
    "                    learning_rate=0.01,  \n",
    "                    colsample_bytree = 0.4,\n",
    "                    subsample = 0.85,\n",
    "                    objective='binary:logistic', \n",
    "                    n_estimators=600, \n",
    "                    reg_alpha = 0.3,\n",
    "                    max_depth=7, \n",
    "                    gamma=1)  \n",
    "clf.fit(train_data_bow_bigram,y_train)\n",
    "y_pred = clf.predict(test_data_bow_bigram)\n",
    "#bigram_pred = clf.predict(test_data_bow_bigram.toarray())\n",
    "print('Training set score:', clf.score(train_data_bow_bigram, y_train_clean))\n",
    "print('\\nTest set score:', clf.score(test_data_bow_bigram, y_test_clean))\n",
    "cross_val = cross_val_score(clf, train_data_bow_bigram, y_train_clean)\n",
    "print('Cross Validation Score:', cross_val)\n",
    "print('Cross Validation Mean:', cross_val.mean())\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for XGB performance visualization on bigrams\n",
    "\n",
    "labels = ['Natural Language Processing', 'Text Mining',\n",
    "         'Pattern Recognition', 'Machine Learning', 'Text Analytics',\n",
    "         'Apache Spark', 'Speech Recognition', 'Neural Networks']\n",
    "\n",
    "conf = confusion_matrix(y_test_clean, y_pred, labels = labels)\n",
    "\n",
    "cm = pd.DataFrame(conf, index = [i for i in labels],\n",
    "                  columns = [i for i in labels])\n",
    "plt.figure(figsize = (15,7))\n",
    "plt.title('Confusion Matrix for BoW with Bigrams XGBoost')\n",
    "sns.heatmap(cm, annot=True, cmap=\"PuBu\", fmt = 'd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(bow)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf = train_test_split(tfidf, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "fit = rfc.fit(X_train_tfidf, y_train)\n",
    "y_pred = rfc.predict(X_test_tfidf)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test_tfidf, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "start = time.time()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf,y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_tfidf, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Normalize the data.\n",
    "X_normt = normalize(tfidf)\n",
    "# Reduce it to two components.\n",
    "X_pcat = PCA(2).fit_transform(X_normt.todense())\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X_pcat)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pcat[:, 0], X_pcat[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "end = time.time()\n",
    "print(\"Done in %0.3fs\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = en_stop.union(set(['machine','business','analytics','learning','technology',\n",
    "                              'system', 'science', 'research', 'analysis', 'experience',\n",
    "                               'customer', 'include', 'plant', 'clerkship', 'language',\n",
    "                               'university', 'google', 'hadoop', 'spark', 'apache', 'microsoft',\n",
    "                               'ai', 'ml', 'nlp', 'speech', 'natural', 'engineers', 'cloud',\n",
    "                               'intelligence', 'aws']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "vectorizer1 = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the job descriptions\n",
    "                             min_df=10, # only use words that appear at least 10x\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case \n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "X_train_tfidf = vectorizer1.fit_transform(X_train_clean)\n",
    "X_test_tfidf = vectorizer1.transform(X_test_clean)\n",
    "\n",
    "fit = rfc.fit(X_train_tfidf, y_train)\n",
    "y_pred = rfc.predict(X_test_tfidf)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test_tfidf, y_test))\n",
    "end = time.time()\n",
    "print(\"Done in %0.3fs\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['title', 'gender_breakdown'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "X = df1\n",
    "X_norm_tfidf = normalize(vectorizer1.fit_transform(X))\n",
    "\n",
    "true_k = 8\n",
    "labels = y \n",
    "km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', batch_size=5000)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X_norm_tfidf)\n",
    "mini_labels = km.labels_\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_norm_tfidf, km.labels_, sample_size=5000))\n",
    "end = time.time()\n",
    "print(\"Done in %0.3fs\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mini_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MiniBatchLabels'] = mini_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(mini_labels):\n",
    "    print('Cluster: %d' % label)\n",
    "    print(df1[df1.MiniBatchLabels == label].groupby('job_class').count())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check Cluster 0 \n",
    "shuffle(df1[df1.MiniBatchLabels == 0]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check Cluster 1\n",
    "shuffle(df1[df1.MiniBatchLabels == 1]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 2\n",
    "shuffle(df1[df1.MiniBatchLabels == 2]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features: %d\" % X_train_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space to about 10% of the original.\n",
    "svd= TruncatedSVD(2100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of job descriptions our solution considers similar, for the first five identified topics\n",
    "jobs_by_component=pd.DataFrame(X_train_lsa, index=list(X_train))\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(jobs_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "import time\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=5,n_iter=500)\n",
    "lsa.fit(tfidf)\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a GBM classifier here, using TF-IDF \n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train_tfidf, y_train)\n",
    "tfidf_pred = clf.predict(X_test_tfidf.toarray())\n",
    "\n",
    "print('Training set score:', clf.score(X_train_tfidf.toarray(), y_train))\n",
    "print('\\nTest set score:', clf.score(X_test_tfidf.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for GBM performance visualization on TF-IDF\n",
    "\n",
    "conf = confusion_matrix(y_test, tfidf_pred, labels = labels)\n",
    "\n",
    "df_cm = pd.DataFrame(conf, index = [i for i in labels],\n",
    "                  columns = [i for i in labels])\n",
    "plt.figure(figsize = (15,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Blues\", fmt = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSA with Tf-idf\n",
    "mlp = neural_network.MLPClassifier(hidden_layer_sizes = (100,), activation = 'logistic')\n",
    "train = mlp.fit(X_train_lsa, y_train)\n",
    "mlp_y_pred = mlp.predict(X_test_lsa)\n",
    "\n",
    "print('Training set score:', mlp.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', mlp.score(X_test_lsa, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
